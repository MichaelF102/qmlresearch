import streamlit as st
import pandas as pd
import re
import os
import tempfile
import time
import gzip
from collections import Counter
from PyPDF2 import PdfReader
import plotly.express as px
import plotly.graph_objects as go
st.set_page_config(
    page_title="RM CIA 2",
    page_icon="üìù",
    layout="wide",
)

# --- Sections dictionary: use keys that match the sidebar exactly ---
SECTIONS = {
    "General": """ """,
    "Abstract": """ """,
    "Introduction": """ """,
    "Research Objective": """ """,
    "Research Questions": """ """,
    "Literature Review": """ """,
    "Methodology": """ """,
    #"Analysis": """ """,
    "Challenges and Limitations": """ """,
    "Case Studies": """ """,
    "Future Directions": """ """,
    "Conclusion": """ """,
    "References": """ """,
    
}

# --- Sidebar ---
st.sidebar.header("Navigation")
section = st.sidebar.selectbox("Select Section", list(SECTIONS.keys()))

# --- Main area ---
st.title("üìâ QML in Finance")

st.markdown(f"### {section}")

if section == "General":
    st.write("""
**Name:** Michael Fernandes

**UID:** 2509006

**Roll No.:** 06

**Subject:** Research Methodology and Statistical Methods

**Title:** Quantum Machine Learning in Finance""")
    
    st.image("img41.png", width='stretch')

if section == "Abstract":
    st.write("""Algorithmic trading and high-frequency trading (HFT) represent the forefront of modern financial markets, where success is dictated by the ability to process massive volumes of data, identify profitable patterns, and execute trades within microseconds. Traditional machine learning (ML) methods have enabled significant advances in predictive modeling, risk assessment, and execution strategies. However, as financial data streams continue to grow in volume, velocity, and variety, the scalability and latency constraints of classical computing pose serious challenges to sustaining competitive advantage in such environments.

This research explores the convergence of **Quantum Machine Learning (QML)** and **Big Data analytics** as a next-generation paradigm for algorithmic and high-frequency trading. QML leverages fundamental principles of quantum mechanics‚Äîsuperposition, entanglement, and quantum parallelism‚Äîto enable computational speedups in optimization, classification, and pattern recognition tasks that are central to financial decision-making. 

In parallel, Big Data technologies such as **Apache Kafka, Spark, and Flink** provide the infrastructure for ingesting, processing, and streaming tick-level order book data, news feeds, and alternative market signals in real time. By integrating QML into Big Data pipelines, we investigate the potential for **hybrid architectures** where classical systems handle large-scale preprocessing, while quantum-enhanced algorithms address computationally intensive tasks such as **portfolio optimization, market regime detection, and ultra-low-latency signal prediction**.

The study evaluates the feasibility of deploying QML models in practical trading scenarios, with emphasis on **latency requirements, scalability, and hardware constraints** of near-term quantum processors. Case studies include the application of **Quantum Support Vector Machines (QSVM)** for market state classification, the **Quantum Approximate Optimization Algorithm (QAOA)** for portfolio allocation under risk constraints, and hybrid reinforcement learning frameworks where quantum policy networks guide high-frequency order execution.

While current quantum hardware limitations‚Äîsuch as decoherence, qubit scarcity, and noise‚Äîprevent full-scale deployment, our analysis suggests that quantum-enhanced approaches can provide measurable advantages in **simulation, optimization, and predictive accuracy**, particularly when integrated with cloud-accessible quantum services. Looking ahead, as quantum computing hardware matures, the fusion of QML and Big Data is poised to redefine the computational foundations of algorithmic trading, enabling faster, smarter, and more adaptive strategies in markets increasingly shaped by data-driven intelligence.
""")
if section == "Introduction":
    st.write("""
Financial markets have transformed dramatically over the last two decades, fueled by advances in computing, data analytics, and communications. Algorithmic trading now dominates equities, futures, and FX markets, while high-frequency trading (HFT) pushes execution into microsecond territory. Competitive advantage depends on processing vast, heterogeneous data streams in real time, extracting predictive signals, and executing decisions at ultra-low latency.

Classical machine learning (ML) has enabled important advances‚Äîprice forecasting, volatility analysis, sentiment detection, and anomaly recognition‚Äîbut struggles with growing data scale and dimensionality. Challenges such as long training times, overfitting, and the inability to handle tick-level throughput limit its effectiveness in modern HFT contexts.

At the same time, Big Data frameworks such as Apache Kafka, Spark, and Flink support ingestion of massive structured and unstructured datasets, from order books and trades to news and social media. These infrastructures address the ‚Äú3 Vs‚Äù of Big Data‚Äîvolume, velocity, variety‚Äîyet optimization tasks like portfolio risk minimization and arbitrage detection still scale poorly on classical systems.

In addition, financial markets are characterized by non-stationarity and fat-tailed distributions that defy many of the assumptions underpinning classical statistical models. Regime shifts, liquidity shocks, and black-swan events amplify the difficulty of predictive modeling, making robust generalization a persistent challenge. Traditional ML methods require extensive retraining to adapt to evolving market dynamics, often lagging behind real-time structural changes.

The complexity is further heightened by the interplay of heterogeneous data modalities. Structured numerical time series from order books must be fused with unstructured text from news feeds and even alternative signals such as satellite imagery or transaction-level ESG data. Extracting actionable insights across these modalities in near real-time requires algorithms that are not only scalable but also capable of uncovering subtle correlations in extremely high-dimensional spaces‚Äîan area where quantum-enhanced approaches may offer distinct advantages.

Against this backdrop, Quantum Machine Learning (QML) emerges as a potential game-changer. By leveraging superposition, entanglement, and quantum parallelism, QML algorithms such as QAOA, VQE, and QSVM can accelerate optimization, classification, and pattern recognition. While today‚Äôs NISQ hardware imposes constraints, hybrid architectures‚Äîwhere classical systems handle scale and quantum processors target complexity‚Äîpoint toward a future of quantum-augmented trading pipelines and, eventually, fully quantum-native financial strategies.

Moreover, the integration of QML into finance is not merely about speed. It represents a paradigm shift in how problems are formulated: optimization landscapes can be explored in fundamentally new ways, feature spaces can be mapped into richer quantum Hilbert spaces, and risk management frameworks can be reimagined through probabilistic interpretations grounded in quantum mechanics. As research progresses, the synergy between quantum and classical systems could reshape the foundations of financial modeling, ushering in an era of innovation that parallels the digital revolution of the early 2000s.
""")
    st.image("img38.png", width='stretch')
    st.image("img37.webp", width='stretch')
    st.image("img36.webp",  width='stretch')
    st.image("img20.jpeg", caption="QML vs Classical ML", width='stretch')
    st.image("img3.webp",  width='stretch')
    st.image("img39.png",  width='stretch')


if section == "Research Objective":
    st.markdown("""
        <h3>1. Investigate the Current State of QML and Big Data in Finance</h3>
        <h3>2. Develop a Hybrid Big Data‚ÄìQML Framework</h3>
        <h3>3. Assess Latency and Scalability Trade-offs</h3>
        <h3>4. Conduct Case Studies and Simulation Experiments</h3>
        <h3>5. Analyze Challenges, Risks, and Adoption Barriers</h3>
        <h3>6. Define a Roadmap for Future Research and Development</h3>
    """, unsafe_allow_html=True)

if section == "Research Questions":
    st.markdown("""
<h4>1.	Algorithmic Performance</h4>
                
‚Ä¢	In which areas of algorithmic trading and HFT do classical machine learning methods fail to meet the demands of scale, latency, or complexity?
                
‚Ä¢	Can QML algorithms (e.g., QAOA, QSVM, QRL) provide measurable improvements in optimization, classification, or pattern recognition tasks?

<h4>2.	System Architecture</h4>

‚Ä¢	How can hybrid architectures be designed where classical Big Data systems manage data volume while quantum processors address computational bottlenecks?

‚Ä¢	What orchestration strategies best integrate Kafka, Spark, or Flink with QML modules?

<h4>3.	Latency and Scalability</h4>
                
‚Ä¢	To what extent do current NISQ devices introduce prohibitive latency for real-time HFT execution?
                
‚Ä¢	Are there mid-frequency or simulation-based use cases where QML can deliver practical benefits despite hardware constraints?
                
‚Ä¢	How does QML performance scale as data dimensionality, asset universes, or feature sets expand?
                
<h4>4.	Case Study Validation</h4>
                
‚Ä¢	How do QML approaches compare with classical baselines in portfolio optimization, regime detection, order execution, and arbitrage detection?
                
‚Ä¢	Do QML-enhanced methods demonstrate robustness in noisy, high-dimensional financial environments?
                
<h4>5.	Adoption Challenges</h4>
                
‚Ä¢	What regulatory, interpretability, and systemic risk issues arise from deploying QML-enhanced trading algorithms?
                
‚Ä¢	How do cost-benefit trade-offs and talent shortages affect the economic viability of QML adoption in finance?
                
<h4>6.	Future Roadmap</h4>
                
‚Ä¢	What short-, medium-, and long-term pathways exist for integrating QML into financial markets?
                
‚Ä¢	What advances in hardware, algorithms, and infrastructure are most critical for transitioning from proofs-of-concept to production-grade trading engines?

    """, unsafe_allow_html=True)

if section == "Literature Review":
    st.markdown("""
<h2>1. Algorithmic and High-Frequency Trading (HFT)</h2>
""", unsafe_allow_html=True)
    st.image("img9.png", use_container_width=True)
    st.markdown("""
<h2>2. Big Data in Financial Trading</h2>
""", unsafe_allow_html=True)
    st.image("img2.png",  use_container_width=True)
    st.markdown("""
<h2>3. Quantum Machine Learning Algorithms</h2>
""", unsafe_allow_html=True)
    st.image("img17.png",  use_container_width=True)
    st.markdown("""
<h2>4. Applications of QML in Finance</h2>
""", unsafe_allow_html=True)
    st.image("img11.png",  use_container_width=True)
    st.markdown("""
<h2>5. Industry Perspectives</h2>
""", unsafe_allow_html=True)
    st.image("img12.webp",  use_container_width=True)
    st.markdown("""
<h2>6. Regulatory, Risk, and Adoption Considerations</h2>
""", unsafe_allow_html=True)
    st.image("img13.webp",  use_container_width=True)
    

if section == "Methodology":
    st.markdown("""
<h2>1. Big Data Pipelines</h2>
""", unsafe_allow_html=True)
    st.image("img14.webp",  use_container_width=True)
    st.markdown("""
<h2>2. Quantum Algorithms in Trading</h2>
                
<h3>1.Quantum Support Vector Machines (QSVM)</h3> 
A QSVM is a classical SVM that uses a quantum kernel: a quantum circuit embeds data x into a high-dimensional Hilbert space.

The circuit estimates similarities via Hadamard/swap-style tests, yielding the kernel (Gram) matrix.

A standard SVM then trains on this matrix to find the separating hyperplane.

Idea: quantum feature maps can produce rich, hard-to-classically-simulate kernels, potentially improving generalization on some problems.

Think of a QSVM as a regular SVM that outsources ‚Äúsimilarity checking‚Äù to a quantum circuit.

We turn each data point into a pattern on qubits (a feature map).

The quantum computer runs a tiny test to measure how alike two points are; do this for all pairs to make a similarity table (kernel).

A normal SVM uses that table to draw the best boundary between classes.

Why bother? The quantum mapping can capture very rich patterns that might be hard for classical methods‚Äîthough today it‚Äôs limited by small, noisy hardware and the cost of many measurements                          
                
Reality today: benefits are constrained by noise, finite shots, and small qubit counts‚Äîso QSVMs are mainly demonstrated on small datasets. 
""", unsafe_allow_html=True)
    st.image("img32.png",  use_container_width=True)
    if "show_code" not in st.session_state:
        st.session_state.show_code = False
    if st.button("QSVM ",key="qsvm_code_button"):
        st.session_state.show_code = not st.session_state.show_code

    # ---- The code to display ----
    QSVM_CODE = r'''
    # QSVM ‚Äî minimal, version-robust implementation (Qiskit + scikit-learn)
    # pip install qiskit qiskit-aer qiskit-machine-learning scikit-learn numpy
    import numpy as np
    from sklearn.preprocessing import StandardScaler
    from sklearn.svm import SVC

    class QSVM:
        def __init__(self, feature_dim, reps=2, shots=2048, C=1.0, seed=7):
            self.reps, self.shots, self.C, self.seed = reps, shots, C, seed
            self._kernel, self._eval = self._build_kernel(feature_dim)
            self.scaler = StandardScaler()
            self.X_train_ = None
            self.clf = SVC(kernel="precomputed", C=C)

        def _build_kernel(self, feature_dim):
            """Return (kernel_object, eval_fn) that computes Gram matrices."""
            try:
                # New API (preferred)
                from qiskit.circuit.library import ZZFeatureMap
                from qiskit_aer.primitives import Sampler
                from qiskit_machine_learning.kernels import FidelityQuantumKernel
                fmap = ZZFeatureMap(feature_dimension=feature_dim, reps=self.reps, entanglement="full")
                sampler = Sampler(options={"shots": self.shots, "seed": self.seed})
                qk = FidelityQuantumKernel(feature_map=fmap, sampler=sampler)
                return qk, (lambda X, Y=None: qk.evaluate(X, Y))
            except Exception:
                # Fallback (older API)
                from qiskit import BasicAer
                from qiskit.circuit.library import ZZFeatureMap
                from qiskit_machine_learning.kernels import QuantumKernel
                backend = BasicAer.get_backend("qasm_simulator")
                fmap = ZZFeatureMap(feature_dimension=feature_dim, reps=self.reps, entanglement="full")
                qk = QuantumKernel(feature_map=fmap, quantum_instance=backend,
                                shots=self.shots, seed_transpiler=self.seed, seed_simulator=self.seed)
                return qk, (lambda X, Y=None: qk.evaluate(x_vec=X, y_vec=Y))

        def fit(self, X, y):
            Xs = self.scaler.fit_transform(X)
            K = self._eval(Xs)                 # (n, n)
            self.clf.fit(K, y)
            self.X_train_ = Xs                 # keep for test-kernel
            return self

        def decision_function(self, X):
            Xs = self.scaler.transform(X)
            K = self._eval(Xs, self.X_train_)  # (m, n)
            return self.clf.decision_function(K)

        def predict(self, X):
            Xs = self.scaler.transform(X)
            K = self._eval(Xs, self.X_train_)
            return self.clf.predict(K)

    # ---- tiny usage example ----
    if __name__ == "__main__":
        # Toy binary dataset
        rng = np.random.default_rng(0)
        X = rng.normal(size=(120, 6))
        y = np.where((X[:, 0] + X[:, 1]**2 - X[:, 2]*X[:, 3]) > 0, 1, -1)

        Xtr, Xte = X[:80], X[80:]
        ytr, yte = y[:80], y[80:]

        model = QSVM(feature_dim=X.shape[1], reps=2, shots=2048, C=1.0).fit(Xtr, ytr)
        preds = model.predict(Xte)
        acc = (preds == yte).mean()
        print(f"Test accuracy: {acc:.3f}")
    '''

    # Show code + offer download
    if st.session_state.show_code:
        st.code(QSVM_CODE, language="python")
        st.download_button("Download qsvm.py", QSVM_CODE, file_name="qsvm.py", mime="text/x-python")

    st.markdown("""

<h3>2.Quantum Approximate Optimization Algorithm (QAOA)</h3> 
It‚Äôs a way to tackle hard ‚Äúpick the best combo‚Äù problems (like splitting a network into two groups) using a small quantum circuit plus a classical optimizer.

You put qubits in a mix of all possibilities, then alternate two simple steps: one nudges the state toward better answers (the ‚Äúcost‚Äù step), the other shuffles options (the ‚Äúmixer‚Äù).

Each step has dial settings (angles). A classical computer tweaks those dials, checking how good the quantum circuit‚Äôs answers look.

After tuning, you measure the circuit many times and keep the best bitstring you see‚Äîthat‚Äôs your approximate solution.

More alternations (depth p) can give better answers, but need longer, noisier circuits and more measurements.
""", unsafe_allow_html=True)
    st.image("img31.png",  use_container_width=True)
    if "show_code2" not in st.session_state:
        st.session_state.show_code2 = False
    if st.button("QAOA ",key="qaoa_code_button"):
        st.session_state.show_code2 = not st.session_state.show_code2

    QAOA_CODE = r'''
    # QSVM ‚Äî minimal, version-robust implementation (Qiskit + scikit-learn)
    # pip install qiskit qiskit-aer qiskit-machine-learning scikit-learn numpy
    import numpy as np
    from sklearn.preprocessing import StandardScaler
    from sklearn.svm import SVC

    class QSVM:
        def __init__(self, feature_dim, reps=2, shots=2048, C=1.0, seed=7):
            self.reps, self.shots, self.C, self.seed = reps, shots, C, seed
            self._kernel, self._eval = self._build_kernel(feature_dim)
            self.scaler = StandardScaler()
            self.X_train_ = None
            self.clf = SVC(kernel="precomputed", C=C)

        def _build_kernel(self, feature_dim):
            """Return (kernel_object, eval_fn) that computes Gram matrices."""
            try:
                # New API (preferred)
                from qiskit.circuit.library import ZZFeatureMap
                from qiskit_aer.primitives import Sampler
                from qiskit_machine_learning.kernels import FidelityQuantumKernel
                fmap = ZZFeatureMap(feature_dimension=feature_dim, reps=self.reps, entanglement="full")
                sampler = Sampler(options={"shots": self.shots, "seed": self.seed})
                qk = FidelityQuantumKernel(feature_map=fmap, sampler=sampler)
                return qk, (lambda X, Y=None: qk.evaluate(X, Y))
            except Exception:
                # Fallback (older API)
                from qiskit import BasicAer
                from qiskit.circuit.library import ZZFeatureMap
                from qiskit_machine_learning.kernels import QuantumKernel
                backend = BasicAer.get_backend("qasm_simulator")
                fmap = ZZFeatureMap(feature_dimension=feature_dim, reps=self.reps, entanglement="full")
                qk = QuantumKernel(feature_map=fmap, quantum_instance=backend,
                                shots=self.shots, seed_transpiler=self.seed, seed_simulator=self.seed)
                return qk, (lambda X, Y=None: qk.evaluate(x_vec=X, y_vec=Y))

        def fit(self, X, y):
            Xs = self.scaler.fit_transform(X)
            K = self._eval(Xs)                 # (n, n)
            self.clf.fit(K, y)
            self.X_train_ = Xs                 # keep for test-kernel
            return self

        def decision_function(self, X):
            Xs = self.scaler.transform(X)
            K = self._eval(Xs, self.X_train_)  # (m, n)
            return self.clf.decision_function(K)

        def predict(self, X):
            Xs = self.scaler.transform(X)
            K = self._eval(Xs, self.X_train_)
            return self.clf.predict(K)

    # ---- tiny usage example ----
    if __name__ == "__main__":
        # Toy binary dataset
        rng = np.random.default_rng(0)
        X = rng.normal(size=(120, 6))
        y = np.where((X[:, 0] + X[:, 1]**2 - X[:, 2]*X[:, 3]) > 0, 1, -1)

        Xtr, Xte = X[:80], X[80:]
        ytr, yte = y[:80], y[80:]

        model = QSVM(feature_dim=X.shape[1], reps=2, shots=2048, C=1.0).fit(Xtr, ytr)
        preds = model.predict(Xte)
        acc = (preds == yte).mean()
        print(f"Test accuracy: {acc:.3f}")
    '''
    if st.session_state.show_code2:
        st.code(QAOA_CODE, language="python")
    st.markdown("""

<h3>3.Variational Quantum Circuits (VQC)</h3> 
It‚Äôs a quantum-plus-classical method to find the lowest energy (ground state) of a system‚Äîthink ‚Äúfind the most relaxed arrangement of a molecule.‚Äù

We build a small quantum circuit with dials (angles). Run it, measure the energy, then a classical optimizer tweaks the dials to make that energy smaller.

Repeat this loop until the energy won‚Äôt go lower‚Äîthe circuit now encodes our best approximation of the ground state.

It‚Äôs used for quantum chemistry and materials (bond energies, reaction paths) because quantum circuits can represent rich quantum states.

Limits today: needs lots of measurements, depends on a good circuit design (‚Äúansatz‚Äù), and is sensitive to noise on current hardware.
                
Start with qubits in ‚à£0‚ü©.Run a small quantum circuit with knobs (angles ùúÉ) ‚Üí it spits out measurement results.

A classical computer takes those results and scores them 
                
The classical optimizer then tweaks the knobs to try to make the score better.

Send the new ùúÉ back to the quantum circuit and repeat the loop.

When the score stops improving, the final Œ∏ and state are your best approximate solution (this is the core of VQE/QAOA-style algorithms).
""", unsafe_allow_html=True)
    st.image("img35.png",  use_container_width=True)
    if "show_code3" not in st.session_state:
        st.session_state.show_code3 = False
    if st.button("VQC ",key="vqc_code_button"):
        st.session_state.show_code3 = not st.session_state.show_code3

    VQC_CODE = r'''
    
    # VQC ‚Äî Variational Quantum Classifier (minimal, version-robust)
# pip install qiskit qiskit-aer qiskit-machine-learning qiskit-algorithms scikit-learn numpy
from __future__ import annotations
import numpy as np
from dataclasses import dataclass
from typing import Optional
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

# ------- Qiskit primitives (Aer if available, else basic) -------
try:
    from qiskit_aer.primitives import Estimator as AerEstimator
    EstimatorImpl = AerEstimator
except Exception:
    from qiskit.primitives import Estimator as BasicEstimator
    EstimatorImpl = BasicEstimator

from qiskit.circuit.library import ZZFeatureMap, RealAmplitudes

# ------- Qiskit ML: QNN + classifier wrapper -------
from qiskit_machine_learning.neural_networks import EstimatorQNN
from qiskit_machine_learning.algorithms.classifiers import NeuralNetworkClassifier

# ------- Optimizer (new and old import paths) -------
try:
    from qiskit_algorithms.optimizers import COBYLA
except Exception:
    try:
        from qiskit.algorithms.optimizers import COBYLA  # older path
    except Exception:
        COBYLA = None  # fallback handled below


@dataclass
class VQC:
    feature_dim: int
    ansatz_reps: int = 2
    shots: Optional[int] = None  # None = exact (if backend supports); else sampling
    seed: int = 7
    maxiter: int = 150

    def __post_init__(self):
        # Feature map encodes classical x into gates
        self.feature_map = ZZFeatureMap(feature_dimension=self.feature_dim, reps=1, entanglement="full")
        # Variational ansatz (trainable parameters)
        self.ansatz = RealAmplitudes(num_qubits=self.feature_dim, reps=self.ansatz_reps, entanglement="full")

        # Compose encoding + ansatz
        circuit = self.feature_map.compose(self.ansatz)

        # Estimator primitive (set shots if provided)
        if self.shots is None:
            self.estimator = EstimatorImpl(options={"seed": self.seed})
        else:
            self.estimator = EstimatorImpl(options={"seed": self.seed, "shots": self.shots})

        # QNN maps inputs -> real number in [-1, 1] (‚ü®Z‚ü© on measured qubits internally)
        self.qnn = EstimatorQNN(
            circuit=circuit,
            input_params=list(self.feature_map.parameters),
            weight_params=list(self.ansatz.parameters),
            estimator=self.estimator,
        )

        # Optimizer (fallback to internal default if COBYLA unavailable)
        optimizer = COBYLA(maxiter=self.maxiter) if COBYLA is not None else None

        # Neural network classifier wrapper (binary cross-entropy by default)
        # one_hot=False so labels are {0,1}
        self.clf = NeuralNetworkClassifier(neural_network=self.qnn, optimizer=optimizer, one_hot=False)

        # Feature scaler
        self.scaler = StandardScaler()

    def fit(self, X: np.ndarray, y: np.ndarray):
        """
        X: (n_samples, feature_dim), y: (n_samples,) with labels in {0,1}
        """
        Xs = self.scaler.fit_transform(X)
        self.clf.fit(Xs, y.astype(int))
        return self

    def predict_proba(self, X: np.ndarray) -> np.ndarray:
        Xs = self.scaler.transform(X)
        # NeuralNetworkClassifier outputs class probabilities if available
        try:
            proba = self.clf.predict_proba(Xs)  # shape (n, 2)
            return proba
        except Exception:
            # Fallback: convert raw outputs in [-1,1] to probabilities (p=(1+z)/2)
            raw = self.clf._neural_network.forward(Xs)  # (n, 1)
            p1 = (1.0 + raw.reshape(-1)) * 0.5
            p0 = 1.0 - p1
            return np.vstack([p0, p1]).T

    def predict(self, X: np.ndarray) -> np.ndarray:
        Xs = self.scaler.transform(X)
        try:
            return self.clf.predict(Xs).astype(int)
        except Exception:
            # Fallback via proba
            proba = self.predict_proba(X)
            return (proba[:, 1] >= 0.5).astype(int)


# ---------------------- Tiny usage example ----------------------
if __name__ == "__main__":
    rng = np.random.default_rng(0)
    n, d = 200, 4  # samples, features (= qubits)
    X = rng.normal(size=(n, d))
    # Nonlinear binary label
    y = (np.sin(X[:, 0]) + X[:, 1] * X[:, 2] - 0.25 * X[:, 3] > 0).astype(int)

    # Train/test split
    n_tr = 140
    Xtr, Xte = X[:n_tr], X[n_tr:]
    ytr, yte = y[:n_tr], y[n_tr:]

    vqc = VQC(feature_dim=d, ansatz_reps=2, shots=None, seed=42, maxiter=120).fit(Xtr, ytr)
    preds = vqc.predict(Xte)
    acc = accuracy_score(yte, preds)
    print(f"Test accuracy: {acc:.3f}")

    # Show a few probabilities
    print("Proba (first 5):")
    print(np.round(vqc.predict_proba(Xte[:5]), 3))

    '''
    if st.session_state.show_code3:
        st.code(VQC_CODE, language="python")
    st.markdown("""

<h3>4.Quantum Reinforcement Learning (QRL)</h3> 
Regular RL = an agent tries actions, gets rewards, and learns a policy to maximize long-term reward.

In QRL, that policy is a small quantum circuit with tunable angles; the state is encoded on qubits.

Run the circuit ‚Üí measurements give a probability over actions; sample one, act, get a reward.

A classical optimizer updates the circuit‚Äôs angles so high-reward actions become more likely (same hybrid loop as VQE/QAOA).

Quantum states can represent rich superpositions and enable different exploration/sampling tricks‚Äîpromising but experimental today due to noise, few qubits, and measurement cost.
""", unsafe_allow_html=True)
    st.image("img34.jpg",  use_container_width=True)
    if "show_code4" not in st.session_state:
        st.session_state.show_code4 = False
    if st.button("QRL ",key="qrl_code_button"):
        st.session_state.show_code4 = not st.session_state.show_code4

    QRL_CODE = r'''
    
# Quantum Reinforcement Learning (QRL) ‚Äî minimal policy-gradient (REINFORCE) with a quantum policy
# pip install qiskit qiskit-aer numpy
from __future__ import annotations
import numpy as np

# --------- Qiskit Sampler (Aer if available, else basic) ----------
try:
    from qiskit_aer.primitives import Sampler as AerSampler
    SamplerImpl = AerSampler
except Exception:
    from qiskit.primitives import Sampler as BasicSampler
    SamplerImpl = BasicSampler

from qiskit import QuantumCircuit
from qiskit.circuit import ParameterVector
from qiskit.circuit.library import RealAmplitudes

# ======================= Contextual Bandit Env =======================
class ContextualBandit:
    """
    Simple 2-action contextual bandit.
    Reward for action=1 is Bernoulli(sigmoid(w¬∑x)), for action=0 it's 1 - that.
    """
    def __init__(self, n_features=2, seed=0):
        self.n_features = n_features
        self.rng = np.random.default_rng(seed)
        # Ground-truth mapping from context to optimal action
        self.w = self.rng.normal(size=n_features)
        self.bias = 0.0

    def sample_context(self):
        # Zero-mean normalized context
        x = self.rng.normal(size=self.n_features)
        x = x / (np.linalg.norm(x) + 1e-9)
        return x

    def reward(self, x, action: int) -> float:
        p1 = 1.0 / (1.0 + np.exp(-(self.w @ x + self.bias)))
        p = p1 if action == 1 else (1.0 - p1)
        return float(self.rng.random() < p)

# ======================= Quantum Policy (2 actions) =======================
class QuantumPolicy2A:
    """
    Bernoulli policy pi(a|x; theta) from a variational quantum circuit.
    - Encode context x with single-qubit RY rotations.
    - Trainable ansatz RealAmplitudes provides parameters theta.
    - Action probability p1 = Pr(qubit0 == 1).
    Gradient uses parameter-shift rule on theta.
    """
    def __init__(self, n_features=2, reps=1, shots=512, seed=7):
        assert n_features >= 1, "Need at least 1 feature/qubit"
        self.nq = n_features
        self.reps = reps
        self.shots = shots
        self.rng = np.random.default_rng(seed)

        # Parameters
        self.x_params = ParameterVector("x", self.nq)           # input angles
        self.theta_params = ParameterVector("th", self.nq * reps * 2)  # trainable
        # Build circuit: feature encoding (RY(x_i)), entangle (CZ ladder), variational layers
        qc = QuantumCircuit(self.nq)
        # Encode features
        for q in range(self.nq):
            qc.ry(self.x_params[q], q)
        # Variational layers
        p = 0
        for _ in range(reps):
            # Entangling CZ ladder
            for i in range(self.nq - 1):
                qc.cz(i, i + 1)
            # Single-qubit trainable rotations
            for q in range(self.nq):
                qc.ry(self.theta_params[p], q); p += 1
            for q in range(self.nq):
                qc.rz(self.theta_params[p], q); p += 1

        self.circuit = qc
        self.ordered_params = list(self.circuit.parameters)
        # Build maps to speed up binding
        self._is_x = {par: (par in set(self.x_params)) for par in self.ordered_params}
        # Sampler primitive
        self.sampler = SamplerImpl(options={"shots": shots, "seed": seed})
        # Initialize trainable parameters small
        self.theta = 0.1 * self.rng.normal(size=len(self.theta_params))

    def _values_vector(self, x, theta):
        # Create parameter vector in the circuit's parameter order
        vals = []
        ix = 0
        it = 0
        for par in self.ordered_params:
            if self._is_x[par]:
                vals.append(float(x[ix]))
                ix += 1
            else:
                vals.append(float(theta[it]))
                it += 1
        return [vals]

    def _p_action1(self, x, theta) -> float:
        """Probability of action=1 (qubit 0 measured to '1')."""
        result = self.sampler.run(
            circuits=[self.circuit],
            parameter_values=self._values_vector(x, theta)
        ).result()
        dist = result.quasi_dists[0]  # dict: bitstring -> probability
        # In Qiskit bitstrings are big-endian: last char corresponds to qubit 0
        p1 = sum(p for b, p in dist.items() if b[-1] == "1")
        # Clip to avoid numerical issues
        return float(np.clip(p1, 1e-6, 1 - 1e-6))

    def action_and_logprob(self, x):
        """Sample action ~ Bernoulli(p1) and return log pi(a|x)."""
        p1 = self._p_action1(x, self.theta)
        a = int(self.rng.random() < p1)
        logp = np.log(p1) if a == 1 else np.log(1.0 - p1)
        return a, p1, logp

    def grad_logprob(self, x, a, p1, shift=np.pi / 2) -> np.ndarray:
        """
        Parameter-shift gradient of log pi(a|x) wrt theta.
        For Bernoulli: d/dŒ∏ log œÄ = (a - p1)/(p1*(1-p1)) * d/dŒ∏ p1
        """
        denom = p1 * (1.0 - p1)
        coeff = (a - p1) / max(denom, 1e-8)
        g = np.zeros_like(self.theta)
        for i in range(len(self.theta)):
            th_plus = self.theta.copy();  th_plus[i] += shift
            th_minus = self.theta.copy(); th_minus[i] -= shift
            p_plus = self._p_action1(x, th_plus)
            p_minus = self._p_action1(x, th_minus)
            dp = 0.5 * (p_plus - p_minus)
            g[i] = coeff * dp
        return g

# ======================= REINFORCE Training Loop =======================
def train_qrl(
    episodes=2000,
    n_features=2,
    lr=0.2,
    lr_decay=0.995,
    gamma=1.0,           # (not used in bandit; kept for API)
    reps=1,
    shots=512,
    seed=0
):
    env = ContextualBandit(n_features=n_features, seed=seed)
    policy = QuantumPolicy2A(n_features, reps=reps, shots=shots, seed=seed)
    baseline = 0.0      # value baseline (EMA of returns)
    beta = 0.01         # baseline update rate
    rng = np.random.default_rng(seed)

    avg_return = 0.0
    for ep in range(1, episodes + 1):
        x = env.sample_context()
        a, p1, logp = policy.action_and_logprob(x)
        r = env.reward(x, a)

        # Policy gradient with baseline
        g = policy.grad_logprob(x, a, p1)
        policy.theta += lr * (r - baseline) * g

        # Update baseline (EMA) and learning rate
        baseline = (1 - beta) * baseline + beta * r
        lr *= lr_decay

        avg_return = avg_return * 0.95 + 0.05 * r
        if ep % 200 == 0:
            # Evaluate greedy performance over 200 contexts
            eval_rewards = []
            for _ in range(200):
                xx = env.sample_context()
                p = policy._p_action1(xx, policy.theta)
                aa = int(p >= 0.5)
                eval_rewards.append(env.reward(xx, aa))
            print(f"Episode {ep:4d} | train avg R~{avg_return:.3f} | eval avg R={np.mean(eval_rewards):.3f}")

    return policy, env

# ======================= Run demo =======================
if __name__ == "__main__":
    policy, env = train_qrl(
        episodes=1500,    # increase for better convergence
        n_features=2,
        lr=0.25,
        reps=1,
        shots=512,
        seed=42
    )

    # Quick sanity check
    rng = np.random.default_rng(123)
    test_rewards = []
    for _ in range(500):
        x = env.sample_context()
        p = policy._p_action1(x, policy.theta)
        a = int(p >= 0.5)  # greedy
        test_rewards.append(env.reward(x, a))
    print(f"Greedy average reward over 500 trials: {np.mean(test_rewards):.3f}")


    '''
    if st.session_state.show_code4:
        st.code(QRL_CODE, language="python")
    st.markdown("""
<h2>3. Latency and Scalability</h2>
""", unsafe_allow_html=True)
    st.image("img19.png",  use_container_width=True)
    

if section == "Analysis":
    st.markdown("""
<h2>1. Evaluating the Need for QML in Trading</h2>
""", unsafe_allow_html=True)
    st.image("img2.png",  use_container_width=True)
    st.markdown("""
<h2>2.Hybrid System Architecture Performance</h2>
""", unsafe_allow_html=True)
    st.image("img2.png",  use_container_width=True)
    st.markdown("""
<h2>3.Latency and Scalability Trade-offs</h2>
""", unsafe_allow_html=True)
    st.image("img2.png",  use_container_width=True)
    st.markdown("""
<h2>4. Case Study Insights</h2>
""", unsafe_allow_html=True)
    st.image("img2.png",  use_container_width=True)
    st.markdown("""
<h2>5. Adoption Barriers</h2>
""", unsafe_allow_html=True)
    st.image("img2.png",  use_container_width=True)
    st.markdown("""
<h2>6. Synthesis</h2>
""", unsafe_allow_html=True)
    st.image("img2.png",  use_container_width=True)

if section == "Challenges and Limitations":
    st.markdown("""
        <h3>1. Quantum Hardware</h3>
        <h3>2. Latency in HFT</h3>
        <h3>3. Big Data Integration</h3>
        <h3>4. Algorithmic Limitations</h3>
        <h3>5. Regulation and Risk</h3>
        <h3>6. Economics and Talent</h3>
        <h3>7. Data Quality and Encoding Bottlenecks</h3>
        <h3>8. Standardization and Benchmarking</h3>
        <h3>9. Security and Cryptographic Risks</h3>
        <h3>10. Market Impact and Systemic Risk</h3>
    """, unsafe_allow_html=True)

if section == "Case Studies":
    st.markdown("""
<h2>1. Portfolio Optimization (QAOA)</h2>
""", unsafe_allow_html=True)
    st.image("img22.jpg",  use_container_width=True)
    st.markdown("""
<h2>2.Market Trend Detection </h2>
""", unsafe_allow_html=True)
    st.image("img29.png",  use_container_width=True)
    st.markdown("""
<h2>3.Order Execution </h2>
""", unsafe_allow_html=True)
    st.image("img25.webp",  use_container_width=True)
    st.markdown("""
<h2>4. Arbitrage Detection </h2>
""", unsafe_allow_html=True)
    st.image("img26.png",  use_container_width=True)
    st.markdown("""
<h2>5. Risk Analysis & Stress Testing </h2>
""", unsafe_allow_html=True)
    st.image("img27.png",  use_container_width=True)
    st.markdown("""
<h2>6. Derivatives Pricing (Quantum PDE Solvers)</h2>
""", unsafe_allow_html=True)
    st.image("img28.jpeg",  use_container_width=True)


if section == "Future Directions":
    st.markdown("""
<h2>1. Short-Term (0‚Äì5 Years): Hybrid & Quantum-Inspired Methods</h2>
""", unsafe_allow_html=True)
    
    st.markdown("""
<h2>2. Medium-Term (5‚Äì10 Years): Domain-Specific Applications (QSVM)</h2>
""", unsafe_allow_html=True)
   
    st.markdown("""
<h2>3. Long-Term (10+ Years): Quantum-Native Trading</h2>
""", unsafe_allow_html=True)
    st.image("img40.png",  use_container_width=True)
    st.image("img30.png",  use_container_width=True)


    
if section == "Conclusion":
    st.markdown("""
The convergence of Quantum Machine Learning (QML) and Big Data is a promising frontier for algorithmic and high-frequency trading (HFT). While classical ML has advanced prediction, risk management, and execution, it increasingly struggles with the scale and latency of high-dimensional, real-time markets.

This paper highlighted how QML can augment Big Data systems through hybrid architectures: classical platforms manage data scale, while quantum processors address optimization, classification, and execution challenges. Case studies with QAOA, QSVM, Quantum RL, and quantum annealing demonstrate early advantages in scalability and robustness, even under NISQ hardware constraints.
                
Significant barriers remain ‚Äî limited qubits, noise, short coherence times, and issues of data encoding, interpretability, and compliance prevent full deployment in live HFT pipelines. Yet the roadmap is clear: short term, QML supports strategy design, risk analysis, and simulations; medium term, it may deliver quantum advantage in risk and anomaly detection; long term, fault-tolerant systems could enable quantum-native trading engines operating in real time.
                
Beyond technical hurdles, the economic and regulatory dimensions of QML adoption are equally important. The high cost of quantum infrastructure may initially restrict access to large financial institutions, potentially widening the competitive gap in global markets. Regulators will need to address concerns around transparency, auditability, and systemic risk, ensuring that quantum acceleration enhances ‚Äî rather than destabilizes ‚Äî financial stability.
                
For practitioners, QML offers both opportunity and caution. Early adopters can experiment with hybrid pipelines for portfolio optimization, fraud detection, and scenario modeling, gaining incremental advantages while building organizational expertise. However, premature reliance on noisy devices or black-box models could introduce new risks into already fragile systems. Responsible innovation will require careful balancing of ambition with rigor.
                
For researchers, the fusion of quantum computing and finance opens new avenues in algorithm design, hybrid orchestration, and cross-disciplinary methods. Quantum-enhanced kernels for time series, variational circuits for market simulation, and quantum-inspired heuristics for large-scale optimization all represent fertile ground for exploration. Collaboration between academia, industry, and regulators will be essential to validate progress and establish shared standards.
                
Ultimately, QML‚Äôs integration with Big Data is not just a computational upgrade but a paradigm shift in financial intelligence. It promises to redefine how strategies are designed, risks are managed, and markets are understood. Unlocking this vision will require sustained progress in hardware, algorithms, infrastructure, and governance ‚Äî but the destination is clear: a self-optimizing, quantum-enhanced financial ecosystem capable of adapting with speed, scale, and foresight beyond classical limits.
""")
if section == "References":
    st.markdown("""
1.	Boucher, A., & Kondratyev, A. (2021). Quantum Machine Learning in Finance: From Theory to Applications. Journal of Financial Data Science, 3(4), 1‚Äì19.
2.	Or√∫s, R., Mugel, S., & Lizaso, E. (2019). Quantum computing for finance: Overview and prospects. Reviews in Physics, 4, 100028.
3.	Rebentrost, P., Mohseni, M., & Lloyd, S. (2014). Quantum support vector machine for big data classification. Physical Review Letters, 113(13), 130503.
4.	Farhi, E., Goldstone, J., & Gutmann, S. (2014). A Quantum Approximate Optimization Algorithm. arXiv preprint arXiv:1411.4028.
5.	Preskill, J. (2018). Quantum Computing in the NISQ era and beyond. Quantum, 2, 79.
6.	Schuld, M., Sinayskiy, I., & Petruccione, F. (2015). An introduction to quantum machine learning. Contemporary Physics, 56(2), 172‚Äì185.
7.	Nielsen, M. A., & Chuang, I. L. (2010). Quantum Computation and Quantum Information (10th Anniversary ed.). Cambridge University Press.
8.	Arute, F. et al. (Google AI Quantum and collaborators) (2019). Quantum supremacy using a programmable superconducting processor. Nature, 574, 505‚Äì510.
9.	Cao, Y., Romero, J., Olson, J. P., Degroote, M., Johnson, P. D., Kieferov√°, M., ‚Ä¶ & Aspuru-Guzik, A. (2019). Quantum chemistry in the age of quantum computing. Chemical Reviews, 119(19), 10856‚Äì10915.
10.	Woerner, S., & Egger, D. J. (2019). Quantum risk analysis. npj Quantum Information, 5(1), 1‚Äì8.
""")